<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://wood-b.github.io/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020 Brandon Wood</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Active Learning on Molecular Systems with GraphDot, xTB, ASE, and scikit-learn</title>
      <link>https://wood-b.github.io/post/active_learn_pe/</link>
      <pubDate>Wed, 11 Mar 2020 22:10:34 -0700</pubDate>
      
      <guid>https://wood-b.github.io/post/active_learn_pe/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Since the end of my PhD, I have been interested in coupling active learning with quantum calculations to explore configuration space in molecular systems. The goal of this post is to demonstrate active learning on a simple system and to put a few ideas out there for people to think about and expand on. Another reason I wanted to make this post is that I think it is ridiculously cool how seamlessly GraphDot/xTB/ASE can work together.&lt;/p&gt;

&lt;p&gt;I want to thank &lt;a href=&#34;https://crd.lbl.gov/departments/computational-science/ccmc/staff/alvarez-fellows/yu-hang-tang/&#34; target=&#34;_blank&#34;&gt;Yu-Hang Tang&lt;/a&gt; for all the help with the graph kernel, none of this would have been possible otherwise. If you want to learn more about the graph kernel, check out this &lt;a href=&#34;http://dx.doi.org/10.1063/1.5078640&#34; target=&#34;_blank&#34;&gt;publication&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any questions or comments shoot me an email bwood@lbl.gov.&lt;/p&gt;

&lt;h2 id=&#34;dependencies&#34;&gt;Dependencies:&lt;/h2&gt;

&lt;p&gt;All dependencies can be pip installed with the exception of xTB, which could be replaced with a different energy calculator if necessary.&lt;br /&gt;
- &lt;a href=&#34;https://graphdot.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;GraphDot&lt;/a&gt;
- &lt;a href=&#34;https://xtb-docs.readthedocs.io/en/latest/contents.html&#34; target=&#34;_blank&#34;&gt;xTB&lt;/a&gt;
- &lt;a href=&#34;https://wiki.fysik.dtu.dk/ase/index.html&#34; target=&#34;_blank&#34;&gt;ASE&lt;/a&gt;
- scikit-learn
- Numpy
- Matplotlib&lt;/p&gt;

&lt;h1 id=&#34;conformations-of-polyethylene&#34;&gt;Conformations of Polyethylene&lt;/h1&gt;

&lt;h2 id=&#34;description-of-the-system&#34;&gt;Description of the system&lt;/h2&gt;

&lt;p&gt;The simple example I chose to explore is learning the energy functional of an ensemble of polyethylene chain conformations. I defined the problem as follows. All chains are made up of 3 monomers — 6 carbon atoms. The rationale for short chains is to keep the degrees of freedom manageable for example purposes. Each chain consists of three C-C-C-C torsion angles and a conformation is defined as a unique set of three torsion angles. I discretized the torsion angle distribution to contain 36 states equally spaced by 10 degrees. The ensemble of conformations is generated by sampling over all of the discrete torsional configurations, so there are ~ 36^3 conformations — some of these are not unique because of symmetry.&lt;/p&gt;

&lt;h2 id=&#34;description-of-the-active-learning-algorithm&#34;&gt;Description of the active learning algorithm&lt;/h2&gt;

&lt;p&gt;The objective is to find a surrogate model for calculating the energy of a chain conformation. In general, an energy evaluation with density functional theory (DFT) or another level of quantum chemistry is computationally expensive, so if we can generate a reasonable energy prediction (or predict another property of interest) with a ML model it will save computational time and expand the systems we can study. In this example I generate a graph representation of the different conformations using GraphDot and then use a graph kernel with scikit-learn’s Gaussian Process Regression (GPR) to predict energies. For this simple example we can easily calculate all the energies using xTB; however, if I wanted to use DFT or look at larger systems that would not be possible. As a result, I wanted to implement an active learning strategy. The active learning algorithm I employ is an iterative process where ~1000 conformations are predicted each step, and the 300 most uncertain conformers are fed back into the training data for the next step. This procedure is intended to ensure that the model sees data that will maximally improve the model each step.&lt;/p&gt;

&lt;h2 id=&#34;imports&#34;&gt;Imports&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import os
import time
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xtb
from xtb import GFN2

import ase
from ase.io import read, write
from ase.units import Hartree
from ase.optimize import BFGSLineSearch
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from graphdot import Graph
from graphdot.graph.adjacency import AtomicAdjacency
from graphdot.kernel.molecular import Tang2019MolecularKernel
from graphdot.kernel.basekernel import KroneckerDelta, SquareExponential, TensorProduct
from graphdot.kernel.marginalized import MarginalizedGraphKernel
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#%matplotlib inline
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generate-dataset-via-torsional-configurations&#34;&gt;Generate dataset via torsional configurations&lt;/h2&gt;

&lt;p&gt;Torsion angles range from 0 to 360 degrees or depending on the convention from -180 to 180 degrees. For the purposes of this demo I am going to use 0 to 360 because it fits naturally with the convention ASE uses.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torsion_angles = np.linspace(0.0, 350.0, num=36)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torsion_angles
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.,
       110., 120., 130., 140., 150., 160., 170., 180., 190., 200., 210.,
       220., 230., 240., 250., 260., 270., 280., 290., 300., 310., 320.,
       330., 340., 350.])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Generate an array of all combinations for 3 torsion angles ~46,000
this includes all combinations, not all will be unique because of symmetry&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tor_combinations = np.zeros((46656, 3))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;count = 0
for i in torsion_angles:
    for j in torsion_angles:
        for k in torsion_angles:
            tor_combinations[count] = [i, j, k]
            count += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read in the polyethylene molecule&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pe_mol = read(&amp;quot;pe_n6.xyz&amp;quot;, format=&amp;quot;xyz&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set the energy calculator&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pe_mol.set_calculator(GFN2())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check how long it takes to calculate the energy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%time pe_mol.get_potential_energy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 172 ms, sys: 62.1 ms, total: 234 ms
Wall time: 336 ms





-543.9429312223907
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;helper-functions-for-generating-data&#34;&gt;Helper functions for generating data&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# this function randomly selects sets of torsional configurations
# it returns a list of lists, where the sample_num is number of lists and sample len is the length of each list
def random_tor_list(tor_combinations, sample_num, sample_len):
    tor_copy = np.copy(tor_combinations)
    np.random.shuffle(tor_copy)
    tor_lists = []
    for i in range(sample_num):
        j = int(i * sample_len)
        k = int(i * sample_len + sample_len)
        tor_lists.append(tor_copy[j:k])
    return tor_lists
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# this function rotates all the torsion angles of the base molecule to the desired angles
def rotate_all_torsions(base_mol, tor_atoms, tor_angles, rot_indices):
    # copy base mol
    rot_mol = base_mol.copy()
    # loop through all the torsion angles in the conformer
    for i, atom in enumerate(tor_atoms):
        rot_mol.set_dihedral(a1=atom[0], a2=atom[1], a3=atom[2], a4=atom[3], 
                             angle=tor_angles[i], indices=rot_indices[i])
    return rot_mol
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_graphs(mols, adj):
    graph_list = [Graph.from_ase(mol, adjacency=adj) for mol in mols]
    return graph_list
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(base_mol, tors_list, tor_atoms, rot_indicies, adj, sample_num):
    mol_list = []
    energy_list = []
    for i, angles in enumerate(tors_list):
        rot_mol = rotate_all_torsions(base_mol, tor_atoms, angles, rot_indicies)
        rot_mol.set_calculator(GFN2())
        energy = rot_mol.get_potential_energy()
        # this if statement limits configurations to under -460.0 eV, so no overlapping atoms/unphysical structures
        # this cutoff includes ~ 90% of the total data 
        if energy &amp;lt; -460.0:
            mol_list.append(rot_mol)
            energy_list.append(energy)
        else:
            continue
        if len(energy_list) == sample_num:
            break
        else:
            continue
    graph_list = generate_graphs(mol_list, adj)
    return graph_list, energy_list
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;specify-atoms-involved-in-each-torsion-angle&#34;&gt;Specify atoms involved in each torsion angle&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# these are specific for this particular molecule and xyz file ordering
pe_n6_tor_atoms = [[0, 1, 5, 8], [1, 5, 8, 11], [5, 8, 11, 14]]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# these are specific for this particular molecule and xyz file ordering
pe_n6_tor_indices=[[8,11,12,13,14,15,16,17,18,19], [11,14,15,16,17,18,19], [14,17,18,19]]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generate-graphs-and-define-the-graph-kernel&#34;&gt;Generate graphs and define the graph kernel&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;adj = AtomicAdjacency(shape=&#39;tent2&#39;, zoom=2.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;def generate_graphs(mols, adj):
    graph_list = []
    for mol in mols:
        graph = Graph.from_ase(mol, adjacency=adj)
        graph_list.append(graph)
    return graph_listdef generate_graphs(mols, adj):
    graph_list = [Graph.from_ase(mol, adjacency=adj) for mol in mols]
    return graph_list&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mol_kernel = Tang2019MolecularKernel(edge_length_scale=0.04, stopping_probability=0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to use scikit-learn GPR we need to to define a MarginalizedGraphKernel object. The kernel defined below is essentiually the same as the Tang2019MolecularKernel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;kernel = MarginalizedGraphKernel(node_kernel=TensorProduct(element=KroneckerDelta(0.2)), 
                                 edge_kernel=TensorProduct(length=SquareExponential(0.04)), 
                                 q=0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;check-graphs-and-visualize-similarity-matrix&#34;&gt;Check graphs and visualize similarity matrix&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try_tors = random_tor_list(tor_combinations, 1, 700)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try_graphs, try_energies = generate_data(pe_mol, try_tors[0], pe_n6_tor_atoms, pe_n6_tor_indices, adj, 500)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len(try_graphs), len(try_energies)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(500, 500)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;R_mol = mol_kernel(try_graphs, lmin=1).astype(np.float)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;D_mol = R_mol.diagonal()**-0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K_mol = D_mol[:, None] * R_mol * D_mol[None, :]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.imshow(K_mol)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x2aaad685ef50&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./active_learning_demo_2_51_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K_mol.max()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.0000000000000002
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K_mol.min()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.8540028342871165
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;R = kernel(try_graphs, lmin=1).astype(np.float)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;D = R.diagonal()**-0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K = D[:, None] * R * D[None, :]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.imshow(K)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x2aab1e472fd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./active_learning_demo_2_57_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K.max()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.0000000000000002
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K.min()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.8540028342871165
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;active-learning-class&#34;&gt;Active learning class&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class active_gpr():
    def __init__(self, kernel, X_train, y_train, X_test, y_test):
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
        self.uncertain = None
        self.gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None, alpha=0.015, normalize_y=True)
        self.metrics = {&amp;quot;step&amp;quot;: [], &amp;quot;rmse&amp;quot;: [], &amp;quot;mae&amp;quot;: [], &amp;quot;r2&amp;quot;: []}
        self.step = 0
        
    def train(self):
        self.gpr.fit(self.X_train, self.y_train)
        y_pred = self.gpr.predict(self.X_test)
        r2 = r2_score(self.y_test, y_pred)
        rmse = mean_squared_error(self.y_test, y_pred, squared=False) 
        mae = mean_absolute_error(self.y_test, y_pred)
        self.metrics[&amp;quot;step&amp;quot;].append(self.step)
        self.metrics[&amp;quot;rmse&amp;quot;].append(rmse)
        self.metrics[&amp;quot;mae&amp;quot;].append(mae)
        self.metrics[&amp;quot;r2&amp;quot;].append(r2)
        self.step += 1
    
    def predict(self, X_new, sample_num=300):
        y_pred, y_std = self.gpr.predict(X_new, return_std=True)
        # np.argsort sorts from min to max so selecting from the end of array gives the
        # the max uncertainty
        uncertain_indexes = np.argsort(y_std)
        self.uncertain = uncertain_indexes[(len(uncertain_indexes) - sample_num):]

    def update_data(self, X, y):
        X_new = np.array(X)[self.uncertain]
        y_new = np.array(y)[self.uncertain]
        X_train = np.concatenate((self.X_train, X_new), axis=0)
        y_train = np.concatenate((self.y_train, y_new), axis=0)
        # shuffle data
        shuffle_ind = np.arange(len(X_train))
        np.random.shuffle(shuffle_ind)
        self.X_train = X_train[shuffle_ind]
        self.y_train = y_train[shuffle_ind]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;active-learning-loop&#34;&gt;Active learning loop&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# define train and test torsion sets
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_test_split(tor_combinations, test_size):
    tor_copy = np.copy(tor_combinations)
    np.random.shuffle(tor_copy)
    # returns the train set and test set
    train_set = tor_copy[test_size:]
    test_set = tor_copy[:test_size]
    return train_set, test_set
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test set size will be 3000 but the energy cutoff will remove ~ 10% of data
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_tor_set, test_tor_set = train_test_split(tor_combinations, test_size=4000)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_tor_list = random_tor_list(train_tor_set, 11, 1500)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train_init, y_train_init = generate_data(pe_mol, train_tor_list[0], pe_n6_tor_atoms, pe_n6_tor_indices, adj, 1000)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_test, y_test = generate_data(pe_mol, test_tor_set, pe_n6_tor_atoms, pe_n6_tor_indices, adj, 3000)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pe_gpr = active_gpr(kernel, X_train_init, y_train_init, X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;active_steps = range(0, 10)
for step in active_steps:
    s_time = time.perf_counter()
    print(&amp;quot;Learning Step: {s}&amp;quot;.format(s=pe_gpr.step))
    print(&amp;quot;Training Data Size: {d}&amp;quot;.format(d=len(pe_gpr.X_train)))
    
    pe_gpr.train()
    X_new, y_new = generate_data(pe_mol, train_tor_list[step + 1], pe_n6_tor_atoms, pe_n6_tor_indices, adj, 1000) 
    pe_gpr.predict(X_new)
    pe_gpr.update_data(X_new, y_new)
    e_time = time.perf_counter()
    
    print(&amp;quot;MAE: {mae:0.3f}&amp;quot;.format(mae=pe_gpr.metrics[&amp;quot;mae&amp;quot;][step]))
    print(&amp;quot;RMSE: {rmse:0.3f}&amp;quot;.format(rmse=pe_gpr.metrics[&amp;quot;rmse&amp;quot;][step]))
    print(&amp;quot;R-squared: {r2:0.5f}&amp;quot;.format(r2=pe_gpr.metrics[&amp;quot;r2&amp;quot;][step]))
    print(&amp;quot;Step Time(s): {t:0.2f}&amp;quot;.format(t=(e_time - s_time)))
    print(&amp;quot;---------- End of Step ----------&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Learning Step: 0
Training Data Size: 1000
      0.5 ms on generating jobs
      0.2 ms on creating output buffer
     14.8 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
     19.0 ms on code generation
    156.0 ms on JIT
      2.8 ms on calculating launch configuration
      5.1 ms on GPU kernel execution
    198.2 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.933
RMSE: 1.602
R-squared: 0.99464
Step Time(s): 50.14
---------- End of Step ----------
Learning Step: 1
Training Data Size: 1300
      0.5 ms on generating jobs
      0.3 ms on creating output buffer
     14.1 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
      9.1 ms on code generation
    161.1 ms on JIT
      2.4 ms on calculating launch configuration
      5.2 ms on GPU kernel execution
    192.5 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.742
RMSE: 1.102
R-squared: 0.99747
Step Time(s): 49.42
---------- End of Step ----------
Learning Step: 2
Training Data Size: 1600
      0.5 ms on generating jobs
      0.3 ms on creating output buffer
     13.7 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
      8.7 ms on code generation
    162.7 ms on JIT
      2.5 ms on calculating launch configuration
      5.4 ms on GPU kernel execution
    193.6 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.637
RMSE: 0.953
R-squared: 0.99810
Step Time(s): 56.18
---------- End of Step ----------
Learning Step: 3
Training Data Size: 1900
      0.5 ms on generating jobs
      0.3 ms on creating output buffer
     14.2 ms on transferring graphs to GPU
      0.2 ms on allocate global job counter
      8.8 ms on code generation
    158.5 ms on JIT
      2.6 ms on calculating launch configuration
      5.3 ms on GPU kernel execution
    189.9 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.578
RMSE: 0.864
R-squared: 0.99844
Step Time(s): 64.12
---------- End of Step ----------
Learning Step: 4
Training Data Size: 2200
      0.7 ms on generating jobs
      0.2 ms on creating output buffer
     19.0 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
     13.1 ms on code generation
    156.0 ms on JIT
      2.8 ms on calculating launch configuration
      5.3 ms on GPU kernel execution
    196.8 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.519
RMSE: 0.794
R-squared: 0.99868
Step Time(s): 72.42
---------- End of Step ----------
Learning Step: 5
Training Data Size: 2500
      0.6 ms on generating jobs
      0.3 ms on creating output buffer
     17.8 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
     10.2 ms on code generation
    149.2 ms on JIT
      2.8 ms on calculating launch configuration
      5.1 ms on GPU kernel execution
    185.5 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.486
RMSE: 0.666
R-squared: 0.99907
Step Time(s): 81.97
---------- End of Step ----------
Learning Step: 6
Training Data Size: 2800
      0.5 ms on generating jobs
      0.2 ms on creating output buffer
     14.4 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
      8.9 ms on code generation
    144.5 ms on JIT
      2.6 ms on calculating launch configuration
      4.9 ms on GPU kernel execution
    175.8 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.455
RMSE: 0.620
R-squared: 0.99920
Step Time(s): 92.25
---------- End of Step ----------
Learning Step: 7
Training Data Size: 3100
      0.5 ms on generating jobs
      0.2 ms on creating output buffer
     14.3 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
      8.8 ms on code generation
    144.6 ms on JIT
      2.2 ms on calculating launch configuration
      4.8 ms on GPU kernel execution
    175.3 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.441
RMSE: 0.586
R-squared: 0.99928
Step Time(s): 103.26
---------- End of Step ----------
Learning Step: 8
Training Data Size: 3400
      0.6 ms on generating jobs
      0.3 ms on creating output buffer
     18.5 ms on transferring graphs to GPU
      0.2 ms on allocate global job counter
     10.3 ms on code generation
    141.6 ms on JIT
      2.5 ms on calculating launch configuration
      4.9 ms on GPU kernel execution
    178.3 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.415
RMSE: 0.567
R-squared: 0.99933
Step Time(s): 114.71
---------- End of Step ----------
Learning Step: 9
Training Data Size: 3700
      0.6 ms on generating jobs
      0.2 ms on creating output buffer
     14.6 ms on transferring graphs to GPU
      0.1 ms on allocate global job counter
      9.1 ms on code generation
    134.6 ms on JIT
      3.1 ms on calculating launch configuration
      4.9 ms on GPU kernel execution
    166.7 ms on calling GPU kernel (overall)
      0.0 ms on collecting result
MAE: 0.415
RMSE: 0.552
R-squared: 0.99936
Step Time(s): 125.99
---------- End of Step ----------
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;plot-mae-and-rmse&#34;&gt;Plot MAE and RMSE&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure()
ax = plt.subplot(111)
ax.plot(range(1,11), pe_gpr.metrics[&amp;quot;mae&amp;quot;])
ax.set_xlabel(&#39;Active Learing Step&#39;, fontsize=15)
ax.set_ylabel(&#39;MAE\n(eV)&#39;, rotation=0, labelpad=30, fontsize=15)
#ax.set_ylim(0.00, 0.14)
ax.spines[&#39;right&#39;].set_visible(False)
ax.spines[&#39;top&#39;].set_visible(False)
#plt.title(&amp;quot;&amp;quot;, fontsize=15)
#ax.legend([], frameon=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./active_learning_demo_2_73_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure()
ax = plt.subplot(111)
ax.plot(range(1,11), pe_gpr.metrics[&amp;quot;rmse&amp;quot;], color=&amp;quot;orange&amp;quot;)
ax.set_xlabel(&#39;Learing Step&#39;, fontsize=15)
ax.set_ylabel(&#39;RMSE\n(eV)&#39;, rotation=0, labelpad=30, fontsize=15)
#ax.set_ylim(0.00, 0.14)
ax.spines[&#39;right&#39;].set_visible(False)
ax.spines[&#39;top&#39;].set_visible(False)
#plt.title(&amp;quot;&amp;quot;, fontsize=15)
#ax.legend([], frameon=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./active_learning_demo_2_74_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Aromaticity as a Guide to Planarity in Conjugated Molecules and Polymers</title>
      <link>https://wood-b.github.io/publication/aromaticity_as_a_guide_to_planarity_in_conjugated_molecules_and_polymers/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 -0800</pubDate>
      
      <guid>https://wood-b.github.io/publication/aromaticity_as_a_guide_to_planarity_in_conjugated_molecules_and_polymers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conformational Entropy as a Means to Control the Behavior of Poly (diketoenamine) Vitrimers In and Out of Equilibrium</title>
      <link>https://wood-b.github.io/publication/conformational-entropy-vitrimers/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 -0700</pubDate>
      
      <guid>https://wood-b.github.io/publication/conformational-entropy-vitrimers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ion Transport and the True Transference Number in Nonaqueous Polyelectrolyte Solutions for Lithium-Ion Batteries</title>
      <link>https://wood-b.github.io/publication/ion-transport-polyelectrolyte-solutions/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 -0700</pubDate>
      
      <guid>https://wood-b.github.io/publication/ion-transport-polyelectrolyte-solutions/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-throughput Chemistry</title>
      <link>https://wood-b.github.io/project/high-throughput-chemistry/</link>
      <pubDate>Wed, 19 Dec 2018 14:15:49 -0800</pubDate>
      
      <guid>https://wood-b.github.io/project/high-throughput-chemistry/</guid>
      <description>&lt;p&gt;With the rise of modern high performance computers, it is possible to run quantum and classical simulations in automated high-throughput fashion. I have contributed to a number of open source repositories that enable high-throughput chemistry and materials science. The software stack includes: &lt;a href=&#34;https://github.com/materialsproject/pymatgen&#34; target=&#34;_blank&#34;&gt;pymatgen&lt;/a&gt;, &lt;a href=&#34;https://github.com/materialsproject/custodian&#34; target=&#34;_blank&#34;&gt;custodian&lt;/a&gt;, and &lt;a href=&#34;https://github.com/hackingmaterials/atomate&#34; target=&#34;_blank&#34;&gt;atomate&lt;/a&gt;. Check out our &lt;a href=&#34;https://doi.org/10.1016/j.commatsci.2017.07.030&#34; target=&#34;_blank&#34;&gt;atomate&lt;/a&gt; publication.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conjugated Polymers</title>
      <link>https://wood-b.github.io/project/conjugated-polymers/</link>
      <pubDate>Wed, 19 Dec 2018 11:28:58 -0800</pubDate>
      
      <guid>https://wood-b.github.io/project/conjugated-polymers/</guid>
      <description>&lt;p&gt;In macromolecules, there are numerous examples where structure dictates function including electronic conductivity of conjugated polymer materials. To elucidate the structure of doped and excited conjugated polymers we developed a multiscale model that captures electronic structure rearrangement and stochastically generates polymer chain conformations.&lt;/p&gt;

&lt;p&gt;Two forthcoming publications will expand upon the impact of doping and excitation on conjugated polymer structure and the model/code (see above) used to generate chain conformations. Please check back early in the new year for updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Elastic Tensor ML</title>
      <link>https://wood-b.github.io/project/elastic-tensor-ml/</link>
      <pubDate>Wed, 19 Dec 2018 11:28:58 -0800</pubDate>
      
      <guid>https://wood-b.github.io/project/elastic-tensor-ml/</guid>
      <description>&lt;p&gt;The Materials Project contains ~87,000 total materials with ~13,500 elastic tensors, which are computationally intensive to calculate from first principles. Although the total number computed will continue to grow, it would be nice to use the data we already have to predict elastic properties such as bulk and shear modulus for materials where the elastic tensor is yet to be calculated. As a result, the goals of this project are to compare machine learning (ML) models for predicting the bulk modulus, and to update the previous model that was trained on a smaller data set. For example Jupyter Notebooks follow the code link above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Elucidating Solvation Structures for Rational Design of Multivalent Electrolytes — A Review</title>
      <link>https://wood-b.github.io/publication/elucidating-solvation-structures-for-rational-design-of-multivalent-electrolytes-a-review/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 -0700</pubDate>
      
      <guid>https://wood-b.github.io/publication/elucidating-solvation-structures-for-rational-design-of-multivalent-electrolytes-a-review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Interplay between Salt Association and the Dielectric Properties of Low Permittivity Electrolytes: The Case of LiPF6 and LiAsF6 in Dimethyl Carbonate</title>
      <link>https://wood-b.github.io/publication/the-interplay-between-salt-association-and-the-dielectric-properties-of-low-permittivity-electrolytes-the-case-of-lipf6-and-liasf6-in-dimethyl-carbonate/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 -0800</pubDate>
      
      <guid>https://wood-b.github.io/publication/the-interplay-between-salt-association-and-the-dielectric-properties-of-low-permittivity-electrolytes-the-case-of-lipf6-and-liasf6-in-dimethyl-carbonate/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Atomate: A High Level Interface to Generate, Execute, and Analyze Computational Materials Science Workflows</title>
      <link>https://wood-b.github.io/publication/atomate-a-high-level-interface-to-generate-execute-and-analyze-computational-materials-science-workflows/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 -0700</pubDate>
      
      <guid>https://wood-b.github.io/publication/atomate-a-high-level-interface-to-generate-execute-and-analyze-computational-materials-science-workflows/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
