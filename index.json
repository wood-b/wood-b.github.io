[{"authors":["Brandon M. Wood"],"categories":[],"content":" Introduction Since the end of my PhD, I have been interested in coupling active learning with quantum calculations to explore configuration space in molecular systems. The goal of this post is to demonstrate active learning on a simple system and to put a few ideas out there for people to think about and expand on. Another reason I wanted to make this post is that I think it is ridiculously cool how seamlessly GraphDot/xTB/ASE can work together.\nI want to thank Yu-Hang Tang for all the help with the graph kernel, none of this would have been possible otherwise. If you want to learn more about the graph kernel, check out this publication.\nIf you have any questions or comments shoot me an email bwood@lbl.gov.\nDependencies: All dependencies can be pip installed with the exception of xTB, which could be replaced with a different energy calculator if necessary.\n- GraphDot - xTB - ASE - scikit-learn - Numpy - Matplotlib\nConformations of Polyethylene Description of the system The simple example I chose to explore is learning the energy functional of an ensemble of polyethylene chain conformations. I defined the problem as follows. All chains are made up of 3 monomers — 6 carbon atoms. The rationale for short chains is to keep the degrees of freedom manageable for example purposes. Each chain consists of three C-C-C-C torsion angles and a conformation is defined as a unique set of three torsion angles. I discretized the torsion angle distribution to contain 36 states equally spaced by 10 degrees. The ensemble of conformations is generated by sampling over all of the discrete torsional configurations, so there are ~ 36^3 conformations — some of these are not unique because of symmetry.\nDescription of the active learning algorithm The objective is to find a surrogate model for calculating the energy of a chain conformation. In general, an energy evaluation with density functional theory (DFT) or another level of quantum chemistry is computationally expensive, so if we can generate a reasonable energy prediction (or predict another property of interest) with a ML model it will save computational time and expand the systems we can study. In this example I generate a graph representation of the different conformations using GraphDot and then use a graph kernel with scikit-learn’s Gaussian Process Regression (GPR) to predict energies. For this simple example we can easily calculate all the energies using xTB; however, if I wanted to use DFT or look at larger systems that would not be possible. As a result, I wanted to implement an active learning strategy. The active learning algorithm I employ is an iterative process where ~1000 conformations are predicted each step, and the 300 most uncertain conformers are fed back into the training data for the next step. This procedure is intended to ensure that the model sees data that will maximally improve the model each step.\nImports import numpy as np import pandas as pd import os import time  import xtb from xtb import GFN2 import ase from ase.io import read, write from ase.units import Hartree from ase.optimize import BFGSLineSearch  from graphdot import Graph from graphdot.graph.adjacency import AtomicAdjacency from graphdot.kernel.molecular import Tang2019MolecularKernel from graphdot.kernel.basekernel import KroneckerDelta, SquareExponential, TensorProduct from graphdot.kernel.marginalized import MarginalizedGraphKernel  #%matplotlib inline import matplotlib.pyplot as plt  from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score from sklearn.metrics import mean_absolute_error from sklearn.metrics import mean_squared_error  Generate dataset via torsional configurations Torsion angles range from 0 to 360 degrees or depending on the convention from -180 to 180 degrees. For the purposes of this demo I am going to use 0 to 360 because it fits naturally with the convention ASE uses.\ntorsion_angles = np.linspace(0.0, 350.0, num=36)  torsion_angles  array([ 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., 110., 120., 130., 140., 150., 160., 170., 180., 190., 200., 210., 220., 230., 240., 250., 260., 270., 280., 290., 300., 310., 320., 330., 340., 350.])  Generate an array of all combinations for 3 torsion angles ~46,000 this includes all combinations, not all will be unique because of symmetry\ntor_combinations = np.zeros((46656, 3))  count = 0 for i in torsion_angles: for j in torsion_angles: for k in torsion_angles: tor_combinations[count] = [i, j, k] count += 1  Read in the polyethylene molecule\npe_mol = read(\u0026quot;pe_n6.xyz\u0026quot;, format=\u0026quot;xyz\u0026quot;)  Set the energy calculator\npe_mol.set_calculator(GFN2())  Check how long it takes to calculate the energy\n%time pe_mol.get_potential_energy()  CPU times: user 172 ms, sys: 62.1 ms, total: 234 ms Wall time: 336 ms -543.9429312223907  Helper functions for generating data # this function randomly selects sets of torsional configurations # it returns a list of lists, where the sample_num is number of lists and sample len is the length of each list def random_tor_list(tor_combinations, sample_num, sample_len): tor_copy = np.copy(tor_combinations) np.random.shuffle(tor_copy) tor_lists = [] for i in range(sample_num): j = int(i * sample_len) k = int(i * sample_len + sample_len) tor_lists.append(tor_copy[j:k]) return tor_lists  # this function rotates all the torsion angles of the base molecule to the desired angles def rotate_all_torsions(base_mol, tor_atoms, tor_angles, rot_indices): # copy base mol rot_mol = base_mol.copy() # loop through all the torsion angles in the conformer for i, atom in enumerate(tor_atoms): rot_mol.set_dihedral(a1=atom[0], a2=atom[1], a3=atom[2], a4=atom[3], angle=tor_angles[i], indices=rot_indices[i]) return rot_mol  def generate_graphs(mols, adj): graph_list = [Graph.from_ase(mol, adjacency=adj) for mol in mols] return graph_list  def generate_data(base_mol, tors_list, tor_atoms, rot_indicies, adj, sample_num): mol_list = [] energy_list = [] for i, angles in enumerate(tors_list): rot_mol = rotate_all_torsions(base_mol, tor_atoms, angles, rot_indicies) rot_mol.set_calculator(GFN2()) energy = rot_mol.get_potential_energy() # this if statement limits configurations to under -460.0 eV, so no overlapping atoms/unphysical structures # this cutoff includes ~ 90% of the total data if energy \u0026lt; -460.0: mol_list.append(rot_mol) energy_list.append(energy) else: continue if len(energy_list) == sample_num: break else: continue graph_list = generate_graphs(mol_list, adj) return graph_list, energy_list  Specify atoms involved in each torsion angle # these are specific for this particular molecule and xyz file ordering pe_n6_tor_atoms = [[0, 1, 5, 8], [1, 5, 8, 11], [5, 8, 11, 14]]  # these are specific for this particular molecule and xyz file ordering pe_n6_tor_indices=[[8,11,12,13,14,15,16,17,18,19], [11,14,15,16,17,18,19], [14,17,18,19]]  Generate graphs and define the graph kernel adj = AtomicAdjacency(shape='tent2', zoom=2.0)  def generate_graphs(mols, adj): graph_list = [] for mol in mols: graph = Graph.from_ase(mol, adjacency=adj) graph_list.append(graph) return graph_listdef generate_graphs(mols, adj): graph_list = [Graph.from_ase(mol, adjacency=adj) for mol in mols] return graph_list\nmol_kernel = Tang2019MolecularKernel(edge_length_scale=0.04, stopping_probability=0.01)  In order to use scikit-learn GPR we need to to define a MarginalizedGraphKernel object. The kernel defined below is essentiually the same as the Tang2019MolecularKernel.\nkernel = MarginalizedGraphKernel(node_kernel=TensorProduct(element=KroneckerDelta(0.2)), edge_kernel=TensorProduct(length=SquareExponential(0.04)), q=0.01)  Check graphs and visualize similarity matrix try_tors = random_tor_list(tor_combinations, 1, 700)  try_graphs, try_energies = generate_data(pe_mol, try_tors[0], pe_n6_tor_atoms, pe_n6_tor_indices, adj, 500)  len(try_graphs), len(try_energies)  (500, 500)  R_mol = mol_kernel(try_graphs, lmin=1).astype(np.float)  D_mol = R_mol.diagonal()**-0.5  K_mol = D_mol[:, None] * R_mol * D_mol[None, :]  plt.imshow(K_mol)  \u0026lt;matplotlib.image.AxesImage at 0x2aaad685ef50\u0026gt;  K_mol.max()  1.0000000000000002  K_mol.min()  0.8540028342871165  R = kernel(try_graphs, lmin=1).astype(np.float)  D = R.diagonal()**-0.5  K = D[:, None] * R * D[None, :]  plt.imshow(K)  \u0026lt;matplotlib.image.AxesImage at 0x2aab1e472fd0\u0026gt;  K.max()  1.0000000000000002  K.min()  0.8540028342871165  Active learning class class active_gpr(): def __init__(self, kernel, X_train, y_train, X_test, y_test): self.X_train = X_train self.y_train = y_train self.X_test = X_test self.y_test = y_test self.uncertain = None self.gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None, alpha=0.015, normalize_y=True) self.metrics = {\u0026quot;step\u0026quot;: [], \u0026quot;rmse\u0026quot;: [], \u0026quot;mae\u0026quot;: [], \u0026quot;r2\u0026quot;: []} self.step = 0 def train(self): self.gpr.fit(self.X_train, self.y_train) y_pred = self.gpr.predict(self.X_test) r2 = r2_score(self.y_test, y_pred) rmse = mean_squared_error(self.y_test, y_pred, squared=False) mae = mean_absolute_error(self.y_test, y_pred) self.metrics[\u0026quot;step\u0026quot;].append(self.step) self.metrics[\u0026quot;rmse\u0026quot;].append(rmse) self.metrics[\u0026quot;mae\u0026quot;].append(mae) self.metrics[\u0026quot;r2\u0026quot;].append(r2) self.step += 1 def predict(self, X_new, sample_num=300): y_pred, y_std = self.gpr.predict(X_new, return_std=True) # np.argsort sorts from min to max so selecting from the end of array gives the # the max uncertainty uncertain_indexes = np.argsort(y_std) self.uncertain = uncertain_indexes[(len(uncertain_indexes) - sample_num):] def update_data(self, X, y): X_new = np.array(X)[self.uncertain] y_new = np.array(y)[self.uncertain] X_train = np.concatenate((self.X_train, X_new), axis=0) y_train = np.concatenate((self.y_train, y_new), axis=0) # shuffle data shuffle_ind = np.arange(len(X_train)) np.random.shuffle(shuffle_ind) self.X_train = X_train[shuffle_ind] self.y_train = y_train[shuffle_ind]  Active learning loop # define train and test torsion sets  def train_test_split(tor_combinations, test_size): tor_copy = np.copy(tor_combinations) np.random.shuffle(tor_copy) # returns the train set and test set train_set = tor_copy[test_size:] test_set = tor_copy[:test_size] return train_set, test_set  # test set size will be 3000 but the energy cutoff will remove ~ 10% of data  train_tor_set, test_tor_set = train_test_split(tor_combinations, test_size=4000)  train_tor_list = random_tor_list(train_tor_set, 11, 1500)  X_train_init, y_train_init = generate_data(pe_mol, train_tor_list[0], pe_n6_tor_atoms, pe_n6_tor_indices, adj, 1000)  X_test, y_test = generate_data(pe_mol, test_tor_set, pe_n6_tor_atoms, pe_n6_tor_indices, adj, 3000)  pe_gpr = active_gpr(kernel, X_train_init, y_train_init, X_test, y_test)  active_steps = range(0, 10) for step in active_steps: s_time = time.perf_counter() print(\u0026quot;Learning Step: {s}\u0026quot;.format(s=pe_gpr.step)) print(\u0026quot;Training Data Size: {d}\u0026quot;.format(d=len(pe_gpr.X_train))) pe_gpr.train() X_new, y_new = generate_data(pe_mol, train_tor_list[step + 1], pe_n6_tor_atoms, pe_n6_tor_indices, adj, 1000) pe_gpr.predict(X_new) pe_gpr.update_data(X_new, y_new) e_time = time.perf_counter() print(\u0026quot;MAE: {mae:0.3f}\u0026quot;.format(mae=pe_gpr.metrics[\u0026quot;mae\u0026quot;][step])) print(\u0026quot;RMSE: {rmse:0.3f}\u0026quot;.format(rmse=pe_gpr.metrics[\u0026quot;rmse\u0026quot;][step])) print(\u0026quot;R-squared: {r2:0.5f}\u0026quot;.format(r2=pe_gpr.metrics[\u0026quot;r2\u0026quot;][step])) print(\u0026quot;Step Time(s): {t:0.2f}\u0026quot;.format(t=(e_time - s_time))) print(\u0026quot;---------- End of Step ----------\u0026quot;)  Learning Step: 0 Training Data Size: 1000 0.5 ms on generating jobs 0.2 ms on creating output buffer 14.8 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 19.0 ms on code generation 156.0 ms on JIT 2.8 ms on calculating launch configuration 5.1 ms on GPU kernel execution 198.2 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.933 RMSE: 1.602 R-squared: 0.99464 Step Time(s): 50.14 ---------- End of Step ---------- Learning Step: 1 Training Data Size: 1300 0.5 ms on generating jobs 0.3 ms on creating output buffer 14.1 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 9.1 ms on code generation 161.1 ms on JIT 2.4 ms on calculating launch configuration 5.2 ms on GPU kernel execution 192.5 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.742 RMSE: 1.102 R-squared: 0.99747 Step Time(s): 49.42 ---------- End of Step ---------- Learning Step: 2 Training Data Size: 1600 0.5 ms on generating jobs 0.3 ms on creating output buffer 13.7 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 8.7 ms on code generation 162.7 ms on JIT 2.5 ms on calculating launch configuration 5.4 ms on GPU kernel execution 193.6 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.637 RMSE: 0.953 R-squared: 0.99810 Step Time(s): 56.18 ---------- End of Step ---------- Learning Step: 3 Training Data Size: 1900 0.5 ms on generating jobs 0.3 ms on creating output buffer 14.2 ms on transferring graphs to GPU 0.2 ms on allocate global job counter 8.8 ms on code generation 158.5 ms on JIT 2.6 ms on calculating launch configuration 5.3 ms on GPU kernel execution 189.9 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.578 RMSE: 0.864 R-squared: 0.99844 Step Time(s): 64.12 ---------- End of Step ---------- Learning Step: 4 Training Data Size: 2200 0.7 ms on generating jobs 0.2 ms on creating output buffer 19.0 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 13.1 ms on code generation 156.0 ms on JIT 2.8 ms on calculating launch configuration 5.3 ms on GPU kernel execution 196.8 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.519 RMSE: 0.794 R-squared: 0.99868 Step Time(s): 72.42 ---------- End of Step ---------- Learning Step: 5 Training Data Size: 2500 0.6 ms on generating jobs 0.3 ms on creating output buffer 17.8 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 10.2 ms on code generation 149.2 ms on JIT 2.8 ms on calculating launch configuration 5.1 ms on GPU kernel execution 185.5 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.486 RMSE: 0.666 R-squared: 0.99907 Step Time(s): 81.97 ---------- End of Step ---------- Learning Step: 6 Training Data Size: 2800 0.5 ms on generating jobs 0.2 ms on creating output buffer 14.4 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 8.9 ms on code generation 144.5 ms on JIT 2.6 ms on calculating launch configuration 4.9 ms on GPU kernel execution 175.8 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.455 RMSE: 0.620 R-squared: 0.99920 Step Time(s): 92.25 ---------- End of Step ---------- Learning Step: 7 Training Data Size: 3100 0.5 ms on generating jobs 0.2 ms on creating output buffer 14.3 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 8.8 ms on code generation 144.6 ms on JIT 2.2 ms on calculating launch configuration 4.8 ms on GPU kernel execution 175.3 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.441 RMSE: 0.586 R-squared: 0.99928 Step Time(s): 103.26 ---------- End of Step ---------- Learning Step: 8 Training Data Size: 3400 0.6 ms on generating jobs 0.3 ms on creating output buffer 18.5 ms on transferring graphs to GPU 0.2 ms on allocate global job counter 10.3 ms on code generation 141.6 ms on JIT 2.5 ms on calculating launch configuration 4.9 ms on GPU kernel execution 178.3 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.415 RMSE: 0.567 R-squared: 0.99933 Step Time(s): 114.71 ---------- End of Step ---------- Learning Step: 9 Training Data Size: 3700 0.6 ms on generating jobs 0.2 ms on creating output buffer 14.6 ms on transferring graphs to GPU 0.1 ms on allocate global job counter 9.1 ms on code generation 134.6 ms on JIT 3.1 ms on calculating launch configuration 4.9 ms on GPU kernel execution 166.7 ms on calling GPU kernel (overall) 0.0 ms on collecting result MAE: 0.415 RMSE: 0.552 R-squared: 0.99936 Step Time(s): 125.99 ---------- End of Step ----------  Plot MAE and RMSE fig = plt.figure() ax = plt.subplot(111) ax.plot(range(1,11), pe_gpr.metrics[\u0026quot;mae\u0026quot;]) ax.set_xlabel('Active Learing Step', fontsize=15) ax.set_ylabel('MAE\\n(eV)', rotation=0, labelpad=30, fontsize=15) #ax.set_ylim(0.00, 0.14) ax.spines['right'].set_visible(False) ax.spines['top'].set_visible(False) #plt.title(\u0026quot;\u0026quot;, fontsize=15) #ax.legend([], frameon=False)  fig = plt.figure() ax = plt.subplot(111) ax.plot(range(1,11), pe_gpr.metrics[\u0026quot;rmse\u0026quot;], color=\u0026quot;orange\u0026quot;) ax.set_xlabel('Learing Step', fontsize=15) ax.set_ylabel('RMSE\\n(eV)', rotation=0, labelpad=30, fontsize=15) #ax.set_ylim(0.00, 0.14) ax.spines['right'].set_visible(False) ax.spines['top'].set_visible(False) #plt.title(\u0026quot;\u0026quot;, fontsize=15) #ax.legend([], frameon=False)  ","date":1583989834,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583989834,"objectID":"d48eefb49622492fcdd3d94ed83259b5","permalink":"https://wood-b.github.io/post/active_learn_pe/","publishdate":"2020-03-11T22:10:34-07:00","relpermalink":"/post/active_learn_pe/","section":"post","summary":"Introduction Since the end of my PhD, I have been interested in coupling active learning with quantum calculations to explore configuration space in molecular systems. The goal of this post is to demonstrate active learning on a simple system and to put a few ideas out there for people to think about and expand on. Another reason I wanted to make this post is that I think it is ridiculously cool how seamlessly GraphDot/xTB/ASE can work together.","tags":[],"title":"Active Learning on Molecular Systems with GraphDot, xTB, ASE, and scikit-learn","type":"post"},{"authors":["Brandon M. Wood","Alexander C. Forse","Kristin A. Persson"],"categories":null,"content":"","date":1581494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581494400,"objectID":"ba0cd4de0dd5885ca687274a0e6a7cf9","permalink":"https://wood-b.github.io/publication/aromaticity_as_a_guide_to_planarity_in_conjugated_molecules_and_polymers/","publishdate":"2020-02-12T00:00:00-08:00","relpermalink":"/publication/aromaticity_as_a_guide_to_planarity_in_conjugated_molecules_and_polymers/","section":"publication","summary":"Conjugated molecules and polymers have the ability to be transformative semiconducting materials; however, to reach their full potential a detailed understanding of the factors governing the molecular structure is crucial for establishing design principles for improved materials. Creating planar or “locking” structures is of particular interest for tuning electronic properties. While noncovalent locks are an effective strategy for increasing planarity, the precise interactions leading to these planar structures are often unknown or mischaracterized. In this study, we demonstrate that aromaticity can be used to investigate, interpret, and modify the complex physical interactions which lead to planarity. Furthermore, we clearly illustrate the important role aromaticity has in determining the structure through torsional preferences and find that modern noncovalent locks utilize hyperconjugation to alter aromaticity and increase planarity. We envision that our approach and our explanation of prevalent noncovalent locks will assist in the design of improved materials for organic electronics.","tags":[],"title":"Aromaticity as a Guide to Planarity in Conjugated Molecules and Polymers","type":"publication"},{"authors":["Changfei He","Peter R. Christensen","Trevor J. Seguin","Eric A. Dailing","Brandon M. Wood","Rebecca K. Walde","Kristin A. Persson","Thomas P. Russell","Brett A. Helms"],"categories":null,"content":"","date":1571122800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571122800,"objectID":"e74f8d012c5622739745d13fea084315","permalink":"https://wood-b.github.io/publication/conformational-entropy-vitrimers/","publishdate":"2019-10-15T00:00:00-07:00","relpermalink":"/publication/conformational-entropy-vitrimers/","section":"publication","summary":"Control of equilibrium and non‐equilibrium thermomechanical behavior of poly(diketoenamine) vitrimers is shown by incorporating linear polymer segments varying in molecular weight (MW) and conformational degrees of freedom into the dynamic covalent network. While increasing MW of linear segments yields a lower storage modulus at the rubbery plateau after softening above the glass transition (Tg), both Tg and the characteristic time of stress relaxation are independently governed by the conformational entropy of the embodied linear segments. Activation energies for bond exchange in the solid state are lower for networks incorporating flexible chains; the network topology freezing temperature decreases with increasing MW of flexible linear segments but increases with increasing MW of stiff segments. Vitrimer reconfigurability is therefore influenced not only by the energetics of bond exchange for a given network density, but also the entropy of polymer chains within the network.","tags":[],"title":"Conformational Entropy as a Means to Control the Behavior of Poly (diketoenamine) Vitrimers In and Out of Equilibrium","type":"publication"},{"authors":["Kara D. Fong","Julian Self","Kyle M. Diederichsen","Brandon M. Wood","Bryan D. McCloskey","Kristin A. Persson"],"categories":null,"content":"","date":1560495600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560495600,"objectID":"5b22c61ecde47405ed348205cf6db4cb","permalink":"https://wood-b.github.io/publication/ion-transport-polyelectrolyte-solutions/","publishdate":"2019-06-14T00:00:00-07:00","relpermalink":"/publication/ion-transport-polyelectrolyte-solutions/","section":"publication","summary":"Nonaqueous polyelectrolyte solutions have been recently proposed as high Li+ transference number electrolytes for lithium ion batteries. However, the atomistic phenomena governing ion diffusion and migration in polyelectrolytes are poorly understood, particularly in nonaqueous solvents. Here, the structural and transport properties of a model polyelectrolyte solution, poly(allyl glycidyl ether-lithium sulfonate) in dimethyl sulfoxide, are studied using all-atom molecular dynamics simulations. We find that the static structural analysis of Li+ ion pairing is insufficient to fully explain the overall conductivity trend, necessitating a dynamic analysis of the diffusion mechanism, in which we observe a shift from largely vehicular transport to more structural diffusion as the Li+ concentration increases. Furthermore, we demonstrate that despite the significantly higher diffusion coefficient of the lithium ion, the negatively charged polyion is responsible for the majority of the solution conductivity at all concentrations, corresponding to Li+ transference numbers much lower than previously estimated experimentally. We quantify the ion–ion correlations unique to polyelectrolyte systems that are responsible for this surprising behavior. These results highlight the need to reconsider the approximations typically made for transport in polyelectrolyte solutions.","tags":[],"title":"Ion Transport and the True Transference Number in Nonaqueous Polyelectrolyte Solutions for Lithium-Ion Batteries","type":"publication"},{"authors":null,"categories":null,"content":"With the rise of modern high performance computers, it is possible to run quantum and classical simulations in automated high-throughput fashion. I have contributed to a number of open source repositories that enable high-throughput chemistry and materials science. The software stack includes: pymatgen, custodian, and atomate. Check out our atomate publication.\n","date":1545257749,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545257749,"objectID":"3523582fb70fc0ed1d9f8011244a715f","permalink":"https://wood-b.github.io/project/high-throughput-chemistry/","publishdate":"2018-12-19T14:15:49-08:00","relpermalink":"/project/high-throughput-chemistry/","section":"project","summary":"Open source python packages for automating molecular workflows","tags":["open source software","high-throughput"],"title":"High-throughput Chemistry","type":"project"},{"authors":null,"categories":null,"content":"In macromolecules, there are numerous examples where structure dictates function including electronic conductivity of conjugated polymer materials. To elucidate the structure of doped and excited conjugated polymers we developed a multiscale model that captures electronic structure rearrangement and stochastically generates polymer chain conformations.\nTwo forthcoming publications will expand upon the impact of doping and excitation on conjugated polymer structure and the model/code (see above) used to generate chain conformations. Please check back early in the new year for updates.\n","date":1545247738,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545247738,"objectID":"3472d54ff05845a9a8b77f4e1d22bc92","permalink":"https://wood-b.github.io/project/conjugated-polymers/","publishdate":"2018-12-19T11:28:58-08:00","relpermalink":"/project/conjugated-polymers/","section":"project","summary":"In macromolecules, there are numerous examples where structure dictates function including electronic conductivity of conjugated polymer materials. To elucidate the structure of doped and excited conjugated polymers we developed a multiscale model that captures electronic structure rearrangement and stochastically generates polymer chain conformations.","tags":["Conjugated Polymers"],"title":"Conjugated Polymers","type":"project"},{"authors":null,"categories":null,"content":"The Materials Project contains ~87,000 total materials with ~13,500 elastic tensors, which are computationally intensive to calculate from first principles. Although the total number computed will continue to grow, it would be nice to use the data we already have to predict elastic properties such as bulk and shear modulus for materials where the elastic tensor is yet to be calculated. As a result, the goals of this project are to compare machine learning (ML) models for predicting the bulk modulus, and to update the previous model that was trained on a smaller data set. For example Jupyter Notebooks follow the code link above.\n","date":1545247738,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545247738,"objectID":"8b32b719c00cc1c361cbd7c8e812da4e","permalink":"https://wood-b.github.io/project/elastic-tensor-ml/","publishdate":"2018-12-19T11:28:58-08:00","relpermalink":"/project/elastic-tensor-ml/","section":"project","summary":"Machine learning models for predicting the bulk modulus of materials","tags":["elastic tensor","machine learning","machine-learning"],"title":"Elastic Tensor ML","type":"project"},{"authors":["Nav Nidhi Rajput","Trevor J. Seguin","Brandon M. Wood","Xiaohui Qu","Kristin A. Persson"],"categories":null,"content":"","date":1524726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524726000,"objectID":"415c06695deca269858cab2bacb65111","permalink":"https://wood-b.github.io/publication/elucidating-solvation-structures-for-rational-design-of-multivalent-electrolytes-a-review/","publishdate":"2018-04-26T00:00:00-07:00","relpermalink":"/publication/elucidating-solvation-structures-for-rational-design-of-multivalent-electrolytes-a-review/","section":"publication","summary":"Fundamental molecular-level understanding of functional properties of liquid solutions provides an important basis for designing optimized electrolytes for numerous applications. In particular, exhaustive knowledge of solvation struc- ture, stability, and transport properties is critical for developing stable electrolytes for fast-charging and high-energy-density next-generation energy storage systems. Accordingly, there is growing interest in the rational design of electrolytes for beyond lithium-ion systems by tuning the molecular-level interactions of solvate species present in the electrolytes. Here we present a review of the solvation struc- ture of multivalent electrolytes and its impact on the electrochemical performance of these batteries. A direct correlation between solvate species present in the solu- tion and macroscopic properties of electrolytes is sparse for multivalent electrolytes and contradictory results have been reported in the literature. This review aims to illustrate the current understanding, compare results, and highlight future needs and directions to enable the deep understanding needed for the rational design of improved multivalent electrolytes.","tags":[],"title":"Elucidating Solvation Structures for Rational Design of Multivalent Electrolytes — A Review","type":"publication"},{"authors":["Julian Self","Brandon M. Wood","Nav Nidhi Rajput","Kristin A. Persson"],"categories":null,"content":"","date":1513756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513756800,"objectID":"3217f03fa08186bf509b1c60d079e71d","permalink":"https://wood-b.github.io/publication/the-interplay-between-salt-association-and-the-dielectric-properties-of-low-permittivity-electrolytes-the-case-of-lipf6-and-liasf6-in-dimethyl-carbonate/","publishdate":"2017-12-20T00:00:00-08:00","relpermalink":"/publication/the-interplay-between-salt-association-and-the-dielectric-properties-of-low-permittivity-electrolytes-the-case-of-lipf6-and-liasf6-in-dimethyl-carbonate/","section":"publication","summary":"In this article, we present evidence that the dielectric constant of an electrolyte solution can be effectively used to infer the association regime of the salt species from computational methods. As case studies, we consider the low dielectric constant solvent dimethyl carbonate with LiAsF6 and LiPF6 salts at low concentrations. Using both quantum “ab initio” methods as well classical molecular dynamics simulations, we elucidate the salt’s contribution to the dielectric constant as well as the dipolar relaxation times, which act as quantitative signatures. By comparing to previously published measurements, we provide strong evidence for the presence of contact-ion pairs at these low concentrations. Interestingly, these ion pairs increase the dielectric constant of the solution, allowing for significantly improved ionic conductivity as a function of salt concentrations. We also discuss the role of multimeric equilibrium species as contributors to the functional properties of designer electrolytes, such as dielectric properties of the solution and ionic conductivity.","tags":[],"title":"The Interplay between Salt Association and the Dielectric Properties of Low Permittivity Electrolytes: The Case of LiPF6 and LiAsF6 in Dimethyl Carbonate","type":"publication"},{"authors":["Kiran Mathew","Joseph H. Montoya","Alireza Faghaninia","Shyam Dwarakanath","Muratahan Aykol","Hanmei Tang","Iek-heng Chu","Tess Smidt","Brandon Bocklund","Matthew Horton","John Dagdelen","Brandon Wood","Zi-kui Liu","Jeffrey Neaton","Shyue Ping","Kristin Persson","Anubhav Jain"],"categories":null,"content":"","date":1501830000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501830000,"objectID":"5fabda5337118f065426f111e51f4455","permalink":"https://wood-b.github.io/publication/atomate-a-high-level-interface-to-generate-execute-and-analyze-computational-materials-science-workflows/","publishdate":"2017-08-04T00:00:00-07:00","relpermalink":"/publication/atomate-a-high-level-interface-to-generate-execute-and-analyze-computational-materials-science-workflows/","section":"publication","summary":"We introduce atomate, an open-source Python framework for computational materials science simula- tion, analysis, and design with an emphasis on automation and extensibility. Built on top of open source Python packages already in use by the materials community such as pymatgen, FireWorks, and custodian, atomate provides well-tested workflow templates to compute various materials properties such as elec- tronic bandstructure, elastic properties, and piezoelectric, dielectric, and ferroelectric properties. Atomate also enables the computational characterization of materials by providing workflows that calculate X-ray absorption (XAS), Electron energy loss (EELS) and Raman spectra. One of the major features of atomate is that it provides both fully functional workflows as well as reusable components that enable one to com- pose complex materials science workflows that use a diverse set of computational tools. Additionally, atomate creates output databases that organize the results from individual calculations and contains a builder framework that creates summary reports for each computed material based on multiple simulations.","tags":[],"title":"Atomate: A High Level Interface to Generate, Execute, and Analyze Computational Materials Science Workflows","type":"publication"}]